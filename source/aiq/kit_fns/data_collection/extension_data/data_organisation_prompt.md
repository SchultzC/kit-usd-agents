Because they are already available in its service we will have mechanism for it to read entire extension source code again here enabling it to have a really precise view on extensions be it a little token effect you know greedy or consuming but but very effective it will be able to read the document Okay so discussions and review on the organizational structure for the data surrounding Omniverse Extensions. So first we are going to have to enable the LLM to ask about specific version of the documentations. Documentation Extensions will evolve over time of said extensions there is overview there is usage there is few other files that we will detect into the documentation folder that we would be able to collect directly that we don't want to duplicate into the database. So inside the extension database we need to do less things as some of these files we will load them directly from the storage. By default if not asked it will just pick up the latest but I might work on a project which is a previous version in which case the database for extensions and their documentation would be different. The second aspect is looking into it there is also different version for the extensions themselves so for a specific version of kit we will lock down a version So here how I'm sort of envisioning the pipeline in a particular kit version we will collect all of the extensions into our you know extension repository data set when extensions are already available we would leave them you know leave them be. We will create then an set but extensions themselves will be different from extension to kit from one another in some case sometimes they will just actually have the identical version in which case we wouldn't want to you know reprocess them or duplicate the storage for them. Finally we index of kind that is sort of a database of these extensions that represent their name their versions some key metrics that we would want to have so we will have a subset of the extension terminal as sort of an entry in that sort of flat database with extension ID its version and then a few other information like want to give the LLM full access to these extensions so part of our data collections will be to have a local repository of all of these extensions on disks. There are a few interesting aspects here is that at any time the LLM will be able to read any file from any extensions easily. a kind of a simple descriptions maybe a bigger descriptions if it has like documentation files and and then few other key metrics that might be useful and then when the APIs that we are using are searching for data they will be able to access these data points through through these files that will be scattered through the information database or the file system database of all of these extensions. Because they are readily available in its service. We will have mechanism for it to read entire extension source code again here enabling it to have really precise view on extensions be it a little token effect you know greedy or consuming but but very effective. It will be able to read the document documentation of set extensions there is overview there is users there is few other files that we will detect into the documentation folder that we would be able to collect directly that we don't want to duplicate into the database. So inside the extension database we need to do less things as some of these files we will load them directly from the storage. So here how I'm sort of envisioning the pipeline. In a particular kit version we will collect all of the extension into our you know extension repository data set when extensions are already available we would leave them you know leave them be. We will create then an index of kind that is sort of a database of these extensions that represent their name their version some key metrics that we would want to have. So we will have a subset of the extension terminal as sort of an entry in that sort of flat database with extension id its version and then a few other informations like kind of a simple descriptions maybe a bigger descriptions if it has like documentation files and then few other key metrics that might be useful and then when the APIs that we are using are searching for data they will be able to access these data points through through these files that will be scattered through the information database or the file system database of all of these extensions.


Based on that sort of information, let's write an overview of the structure of the data we want. And then we will just update a sort of extension_database.md. Again think about the database as a mix of a JSON index file that has the high-level, complex overview and links or flags, and then a storage, a file system storage back-end that includes all of these extensions and the actual larger files that will be loaded on demand.

So think ultra hard make an interesting review of that. Remember to search the web and write this extension_database.md